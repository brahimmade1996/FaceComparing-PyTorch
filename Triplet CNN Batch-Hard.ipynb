{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet CNN Batch-Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Imaging Library\n",
    "from PIL import Image    \n",
    "import PIL.ImageOps\n",
    "\n",
    "# Packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import optim     # 包含optimization algorithms\n",
    "import torch.nn.functional as F     # 包含activation functions\n",
    "from torch.autograd import Variable      # 以Variable形式嵌套激励函数 \n",
    "import torch.nn as nn\n",
    "\n",
    "# Torchvision 包含目前流行的数据集，模型结构和常用的图片转换工具等\n",
    "import torchvision\n",
    "import torchvision.datasets as dset    # 包含一些数据集\n",
    "import torchvision.transforms as transforms  # 可对PIL.Image, Tensor进行变换\n",
    "import torchvision.utils  ##\n",
    "\n",
    "# Others\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import linecache\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    training_dir = \"./Datasets/att/training/\"\n",
    "    validation_dir = \"./Datasets/att/validamulti/\"\n",
    "    testing_dir = \"./Datasets/att/testing/\"\n",
    "    batch_ap_path = 'batch_ap_mixed.txt'\n",
    "    batch_an_path = 'batch_an_mixed.txt'\n",
    "    batch_ap_hard_path = './batch_ap_hard_mixed.txt'\n",
    "    batch_an_hard_path = './batch_an_hard_mixed.txt'\n",
    "    train_batch_size = 32           #批样本数\n",
    "    train_number_epoch = 400     #整批训练次数，即遍历了多少次所有的训练样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, text=None, should_save=False):     #数据集出图\n",
    "    npimg = img.numpy()   #转numpy\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic', fontweight='bold',\n",
    "                bbox={'facecolor':'white','alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "        \n",
    "def show_plot(iteration, loss):                    #观察迭代损失\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.xlabel('Iteration number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all possible AP and AN in mini_batch, and add an all-zero \"helper pic\" in order to input into the TriNet. \n",
    "Write into a .txt to be loaded by DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniBatchAllPairs(data_path, batch_ap_path, batch_an_path, num_class):\n",
    "    helper_path = './ceshi/pic_helper.png'\n",
    "    name_list = os.listdir(data_path)\n",
    "    name_list.sort()\n",
    "    batch_name = random.sample(name_list, num_class)\n",
    "    print(batch_name)\n",
    "    batch_ap = []\n",
    "    for k in range(len(batch_name)):\n",
    "        name = batch_name[k]\n",
    "        path1 = data_path + name + '/'\n",
    "        pic_list1 = os.listdir(path1)\n",
    "        pic_list1.sort()\n",
    "        for i in range(len(pic_list1)):\n",
    "            anchor_pic = pic_list1[i]\n",
    "            anchor_pic = path1 + anchor_pic\n",
    "            for j in range(len(pic_list1)):\n",
    "                if i!= j:\n",
    "                    positive_pic = pic_list1[j]\n",
    "                    positive_pic = path1 + positive_pic\n",
    "                    pic_pair1 = anchor_pic + '\\t' + positive_pic + '\\t' + helper_path\n",
    "                    batch_ap.append(pic_pair1)\n",
    "    \n",
    "    fileObject = open(batch_ap_path, 'w')  \n",
    "    for pic_pair1 in batch_ap:  \n",
    "        fileObject.write(pic_pair1)  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close()\n",
    "    \n",
    "    batch_an = []\n",
    "    for k in range(len(batch_name)):\n",
    "        name = batch_name[k]\n",
    "        path1 = data_path + name + '/'\n",
    "        pic_list2 = os.listdir(path1)\n",
    "        pic_list2.sort()\n",
    "        for i in range(len(pic_list2)):\n",
    "            anchor_pic = pic_list2[i]\n",
    "            anchor_pic = path1 + anchor_pic\n",
    "            for j in range(len(batch_name)):\n",
    "                name_other = batch_name[j]\n",
    "                path2 = data_path + name_other + '/'\n",
    "                pic_list3 = os.listdir(path2)\n",
    "                pic_list3.sort()\n",
    "                if j != k :\n",
    "                    for q in range(len(pic_list3)):\n",
    "                        negative_pic = pic_list3[q]\n",
    "                        negative_pic = path2 + negative_pic\n",
    "                        pic_pair2 = anchor_pic + '\\t'+ negative_pic + '\\t' + helper_path\n",
    "                        batch_an.append(pic_pair2)\n",
    "\n",
    "    fileObject = open(batch_an_path, 'w')  \n",
    "    for pic_pair2 in batch_an:  \n",
    "        fileObject.write(pic_pair2)  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_batch_size = 3    \n",
    "MiniBatchAllPairs(data_path = Config.training_dir,  \n",
    "                  batch_ap_path = Config.batch_ap_path, \n",
    "                  batch_an_path = Config.batch_an_path, \n",
    "                  num_class = class_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('L')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load A-P-helper Triplet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triplet网络要求三个输入，这里我们利用一个全1图像作为“helper”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBatchPairs_AP(Dataset):\n",
    "    def __init__(self, img_path, txt_path, data_transforms=None, loader = default_loader):\n",
    "        with open(txt_path) as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            self.img_anchor = [os.path.join(img_path, line.strip().split('\\t')[0]) for line in lines]\n",
    "            self.img_sample = [os.path.join(img_path, line.strip().split('\\t')[1]) for line in lines]   \n",
    "            self.img_helper = [os.path.join(img_path, line.strip().split('\\t')[2]) for line in lines]\n",
    "        self.data_transforms = data_transforms\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_helper)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_anchor = self.img_anchor[item]\n",
    "        img_sample = self.img_sample[item]\n",
    "        img_helper = self.img_helper[item]\n",
    "        img_a = self.loader(img_anchor)\n",
    "        img_s = self.loader(img_sample)\n",
    "        img_h = self.loader(img_helper)\n",
    "        \n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "                img_a = self.data_transforms(img_a)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_a))\n",
    "            try:\n",
    "                img_s = self.data_transforms(img_s)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_s))\n",
    "            try:\n",
    "                img_h = self.data_transforms(img_h)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_h))   \n",
    "                \n",
    "        return img_a, img_s, img_h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load A-helper-N triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBatchPairs_AN(Dataset):\n",
    "    def __init__(self, img_path, txt_path, data_transforms=None, loader = default_loader):\n",
    "        with open(txt_path) as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            self.img_anchor = [os.path.join(img_path, line.strip().split('\\t')[0]) for line in lines]\n",
    "            self.img_sample = [os.path.join(img_path, line.strip().split('\\t')[1]) for line in lines]   \n",
    "            self.img_helper = [os.path.join(img_path, line.strip().split('\\t')[2]) for line in lines]\n",
    "        self.data_transforms = data_transforms\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_helper)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_anchor = self.img_anchor[item]\n",
    "        img_sample = self.img_sample[item]\n",
    "        img_helper = self.img_helper[item]\n",
    "        img_a = self.loader(img_anchor)\n",
    "        img_s = self.loader(img_sample)\n",
    "        img_h = self.loader(img_helper)\n",
    "        \n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "                img_a = self.data_transforms(img_a)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_a))\n",
    "            try:\n",
    "                img_s = self.data_transforms(img_s)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_s))\n",
    "            try:\n",
    "                img_h = self.data_transforms(img_h)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_h))    \n",
    "                \n",
    "        return img_a, img_h, img_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Triplets  (the Batch-Hardest ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBatchHardest(Dataset):\n",
    "    def __init__(self, img_path, txt_path1, txt_path2, data_transforms=None, loader = default_loader):\n",
    "        with open(txt_path1) as input_file1:\n",
    "            lines = input_file1.readlines()\n",
    "            self.img_anchor = [os.path.join(img_path, line.strip().split('\\t')[0]) for line in lines]\n",
    "            self.img_positi = [os.path.join(img_path, line.strip().split('\\t')[1]) for line in lines]   \n",
    "        with open(txt_path2) as input_file2:\n",
    "            lines = input_file2.readlines()\n",
    "            self.img_negati = [os.path.join(img_path, line.strip().split('\\t')[1]) for line in lines]   \n",
    "        self.data_transforms = data_transforms\n",
    "        self.loader = loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_anchor)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_anchor = self.img_anchor[item]\n",
    "        img_positi = self.img_positi[item]\n",
    "        img_negati = self.img_negati[item]\n",
    "        img_a = self.loader(img_anchor)\n",
    "        img_p = self.loader(img_positi)\n",
    "        img_n = self.loader(img_negati)\n",
    "        \n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "                img_a = self.data_transforms(img_a)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_a))\n",
    "            try:\n",
    "                img_p = self.data_transforms(img_p)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_s))\n",
    "            try:\n",
    "                img_n = self.data_transforms(img_n)\n",
    "            except:\n",
    "                print(\"Cannot transform image: {}\".format(img_h))    \n",
    "                \n",
    "        return img_a, img_p, img_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some APH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ap_datasets = GetBatchPairs_AP(img_path='./',\n",
    "                            txt_path=('./batch_ap_mixed.txt'),\n",
    "                            data_transforms=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                     transforms.ToTensor()\n",
    "                                                                     ]),\n",
    "                            )\n",
    "ap_dataloaders = DataLoader(batch_ap_datasets,\n",
    "                        batch_size=8,\n",
    "                        shuffle=False)\n",
    "ap_data_iter = iter(ap_dataloaders)    #生成迭代器\n",
    "\n",
    "example_batch = next(ap_data_iter)     #返回迭代器的下一个项目\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1],example_batch[2]),0)  #张量拼接，0：按行，1：按列\n",
    "imshow(torchvision.utils.make_grid(concatenated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some AHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_an_datasets = GetBatchPairs_AN(img_path='./',\n",
    "                            txt_path=('./batch_an_mixed.txt'),\n",
    "                            data_transforms=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                     transforms.ToTensor()\n",
    "                                                                     ]),\n",
    "                            )\n",
    "an_dataloaders = DataLoader(batch_an_datasets,\n",
    "                        batch_size=8,\n",
    "                        shuffle=False)\n",
    "an_data_iter = iter(an_dataloaders)    #生成迭代器\n",
    "\n",
    "example_batch = next(an_data_iter)     #返回迭代器的下一个项目\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1],example_batch[2]),0)  #张量拼接，0：按行，1：按列\n",
    "imshow(torchvision.utils.make_grid(concatenated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetwork(nn.Module):                 # torch.nn.Module:所有神经网络模块的基类\n",
    "    def __init__(self):\n",
    "        super(TripletNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(               # torch.nn.Sequential:模块将按照在构造器中的顺序传递\n",
    "            nn.ReflectionPad2d(1),               # 使用输入边界的反射来填充输入张量\n",
    "            nn.Conv2d(                           # 对信号（由若干输入平面组成）进行2维卷积\n",
    "                in_channels=1,                   # input height. 灰度图1层，RGB为3层，即图像的通道数\n",
    "                out_channels=4,                  # n_filters 过滤器数目，过滤器提取的特征数\n",
    "                kernel_size=3,                   # 卷积核 filter size 3x3\n",
    "                stride=1,                        # filter movement/step 跳度\n",
    "                padding=0,                       # 零填充。若想要con2d出来的图片长宽没有变化，当stride=1,padding=(kernel_size-1)/2\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),               # ReLU(x)=max(0,x), inplace不是很懂\n",
    "            nn.BatchNorm2d(4),                   # 对4维输入应用批标准化（一个小批量的2维输入和额外的通道尺寸）？\n",
    "            nn.Dropout2d(p=.2),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(\n",
    "                in_channels=4,\n",
    "                out_channels=8,\n",
    "                kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout2d(p=.2),\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(                   # fc full connected\n",
    "            nn.Linear(in_features=8*100*100,        # 线性层,y=Ax+b\n",
    "                      out_features=500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(500,50)\n",
    "    )\n",
    "    \n",
    "    def forward_once(self,x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0],-1)   #Morvan说是展平多维的卷积图。check:http://pytorch.org/docs/master/tensors.html?highlight=view#torch.Tensor.view\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input1, input2, input3):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        output3 = self.forward_once(input3)\n",
    "        return output1,output2,output3\n",
    "    \n",
    "net = TripletNetwork()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('tri_batch_mixed_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = F.triplet_margin_loss()   \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0005)\n",
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "class_batch_size = 5\n",
    "alpha = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0,Config.train_number_epoch):\n",
    "# for epoch in range(0,100):\n",
    "\n",
    "\n",
    "#先筛选batch内hardest a-p\n",
    "    MiniBatchAllPairs(data_path = Config.training_dir,  \n",
    "                  batch_ap_path = Config.batch_ap_path, \n",
    "                  batch_an_path = Config.batch_an_path, \n",
    "                  num_class = class_batch_size)\n",
    "    batch_ap_datasets = GetBatchPairs_AP(img_path = './',\n",
    "                                         txt_path = Config.batch_ap_path,\n",
    "                                         data_transforms=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                             transforms.ToTensor()\n",
    "                                                                            ])\n",
    "                                        )\n",
    "\n",
    "    gethard_ap_dataloader = DataLoader(batch_ap_datasets,\n",
    "                            shuffle=False,\n",
    "                            batch_size=1)\n",
    "\n",
    "    eu_dist_np = np.array([])\n",
    "    batch_ap_hard = []\n",
    "    for i, data in enumerate(gethard_ap_dataloader,0):                        \n",
    "        img0, img1 , img2 = data\n",
    "        img0, img1 , img2 = Variable(img0), Variable(img1) , Variable(img2)\n",
    "        output1,output2,output3 = net(img0,img1,img2)\n",
    "        eu_distance = F.pairwise_distance(output1, output2)\n",
    "        eu_dist = eu_distance.cpu().data.numpy()[0][0]\n",
    "    #     print(i,eu_dist)\n",
    "        eu_dist_np = np.append(eu_dist_np,eu_dist)\n",
    "        if i!= 0 and (i+1)%9 == 0:\n",
    "            hardest_num = np.argmax(eu_dist_np)\n",
    "            hardest_num = i - 8 + int(hardest_num)    #hardest pair position\n",
    "            linecache.updatecache(Config.batch_ap_path)      #神坑，linecache会缓存首次打开的txt，导致输出一直是首次batch\n",
    "            hardest_ap_line = linecache.getline(Config.batch_ap_path,hardest_num+1)\n",
    "            hardest_ap_line = hardest_ap_line.split()\n",
    "            hardest_ap_line = hardest_ap_line[0] + '\\t' + hardest_ap_line[1]\n",
    "    #         print(hardest_ap_line)\n",
    "            batch_ap_hard.append(hardest_ap_line)\n",
    "            eu_dist_np = np.array([])\n",
    "        \n",
    "    fileObject = open(Config.batch_ap_hard_path, 'w')                   #写入txt\n",
    "    for hardest_ap in batch_ap_hard:\n",
    "        fileObject.write(hardest_ap)  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close()\n",
    "\n",
    "    \n",
    "#筛选batch内hardest a-n\n",
    "    batch_an_datasets = GetBatchPairs_AN(img_path='./',\n",
    "                                         txt_path=Config.batch_an_path,\n",
    "                                         data_transforms=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                         transforms.ToTensor()\n",
    "                                                                   ]),\n",
    "                                        )\n",
    "    gethard_an_dataloader = DataLoader(batch_an_datasets,\n",
    "                        shuffle=False,\n",
    "                        batch_size=1)\n",
    "    eu_dist_np = np.array([])\n",
    "    batch_an_hard = []\n",
    "\n",
    "\n",
    "    for i, data in enumerate(gethard_an_dataloader,0):               #\n",
    "        img0, img1 , img2 = data\n",
    "        img0, img1 , img2 = Variable(img0), Variable(img1) , Variable(img2)\n",
    "        output1,output2,output3 = net(img0,img1,img2)\n",
    "        eu_distance = F.pairwise_distance(output1, output3)\n",
    "        eu_dist = eu_distance.cpu().data.numpy()[0][0]\n",
    "    #     print(i,eu_dist)\n",
    "        eu_dist_np = np.append(eu_dist_np,eu_dist)\n",
    "        if i!= 0 and (i+1)%(10*(class_batch_size-1)) == 0:\n",
    "            hardest_num = np.argmin(eu_dist_np)\n",
    "            hardest_num = i - (10*(class_batch_size-1)-1) + int(hardest_num)    #hardest pair position\n",
    "            linecache.updatecache(Config.batch_an_path)\n",
    "            hardest_an_line = linecache.getline(Config.batch_an_path,hardest_num+1)\n",
    "            hardest_an_line = hardest_an_line.split()\n",
    "            hardest_an_line = hardest_an_line[0] + '\\t' + hardest_an_line[1]\n",
    "#             print(hardest_an_line)\n",
    "            batch_an_hard.append(hardest_an_line)\n",
    "            eu_dist_np = np.array([])\n",
    "        \n",
    "    fileObject = open(Config.batch_an_hard_path, 'w')\n",
    "    for hardest_an in batch_an_hard:\n",
    "        fileObject.write(hardest_an)  \n",
    "        fileObject.write('\\n')  \n",
    "    fileObject.close()\n",
    "\n",
    "#对batch内每个样本生成hardest a-p-n 即hardest triplet,进行训练\n",
    "    batch_hard_datasets = GetBatchHardest(img_path='./',\n",
    "                                          txt_path1=Config.batch_ap_hard_path,\n",
    "                                          txt_path2=Config.batch_an_hard_path,\n",
    "                                          data_transforms=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                              transforms.ToTensor()\n",
    "                                                                             ])\n",
    "                                          )\n",
    "\n",
    "    hard_dataloaders = DataLoader(batch_hard_datasets,\n",
    "                        batch_size=30,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True)\n",
    "\n",
    "    for i, data in enumerate(hard_dataloaders,0):\n",
    "        img0, img1 , img2 = data\n",
    "        img0, img1 , img2 = Variable(img0), Variable(img1) , Variable(img2)\n",
    "        output1,output2,output3 = net(img0,img1,img2)\n",
    "        optimizer.zero_grad()\n",
    "        loss_triplet = F.triplet_margin_loss(output1,output2,output3,margin=alpha)\n",
    "        loss_triplet.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch: {} | Current loss {}\\n\".format(epoch,loss_triplet.data[0]))\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_triplet.data[0])\n",
    "        \n",
    "        \n",
    "    torch.save(net.state_dict(),'tri_batch_mixed_params.pkl')\n",
    "show_plot(counter,loss_history)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'tri_batch_mixed_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('tri_batch_mixed_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)   #在self...imgs中返回随机项\n",
    "        # 先取A-P对，保持循环，直到找到相同的类的图像\n",
    "        while True:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)   #元组=（图片路径，类编号）\n",
    "            if img0_tuple[1] == img1_tuple[1]:                         #图片编号相同则是同一个类（以文件夹区分）\n",
    "                break\n",
    "        # 再取A-N\n",
    "        while True:\n",
    "            img2_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "            if img0_tuple[1] != img2_tuple[1]:\n",
    "                break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])    #读取实际图像，tuple[0]即路径\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img2 = Image.open(img2_tuple[0])\n",
    "        img0 = img0.convert(\"L\")            #from PIL，把img转换为256级灰度图像， L：8-bit pixels,black and white\n",
    "        img1 = img1.convert(\"L\")\n",
    "        img2 = img2.convert(\"L\")\n",
    "\n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)    #将输入图像转换为反色图像\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "            img2 = PIL.ImageOps.invert(img2)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        return img0, img1, img2\n",
    "            #返回 img0,img1,img2，完成A-P-N组合\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)          #数据集大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察训练集分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset_train = dset.ImageFolder(root=Config.training_dir)\n",
    "triplet_dataset = TripletNetworkDataset(imageFolderDataset=folder_dataset_train,\n",
    "                                       transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                    transforms.ToTensor()\n",
    "                                                                    ]),\n",
    "                                       should_invert=False)\n",
    "\n",
    "train_distri_dataloader = DataLoader(triplet_dataset, \n",
    "                             num_workers=6, \n",
    "                             batch_size=1, \n",
    "                             shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "same_one_train = np.array([])\n",
    "diff_one_train = np.array([])\n",
    "\n",
    "\n",
    "for i, data in enumerate(train_distri_dataloader, 0):   #将一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标\n",
    "    img0, img1, img3 = data\n",
    "    img0, img1, img3 = Variable(img0), Variable(img1), Variable(img3)\n",
    "    output1, output2, output3 = net(img0, img1,img3)   # network output\n",
    "    eu_distance_same = F.pairwise_distance(output1, output2)\n",
    "    eu_dist_same = eu_distance_same.cpu().data.numpy()[0][0]\n",
    "    eu_distance_diff = F.pairwise_distance(output1, output3)\n",
    "    eu_dist_diff = eu_distance_diff.cpu().data.numpy()[0][0]\n",
    "    same_one_train = np.append(same_one_train, eu_dist_same)\n",
    "    diff_one_train = np.append(diff_one_train, eu_dist_diff)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(same_one_train, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from same classes')\n",
    "sns.distplot(diff_one_train, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from different classes')\n",
    "plt.xlabel('Dissimilarity(Euclidean Distance)')\n",
    "plt.ylabel('Relative quantity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset_valida = dset.ImageFolder(root=Config.validation_dir)\n",
    "triplet_dataset = TripletNetworkDataset(imageFolderDataset=folder_dataset_valida,\n",
    "                                       transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                    transforms.ToTensor()\n",
    "                                                                    ]),\n",
    "                                       should_invert=False)\n",
    "\n",
    "valida_distri_dataloader = DataLoader(triplet_dataset, \n",
    "                             num_workers=6, \n",
    "                             batch_size=1, \n",
    "                             shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "same_one_valida = np.array([])\n",
    "diff_one_valida= np.array([])\n",
    "\n",
    "\n",
    "for i, data in enumerate(valida_distri_dataloader, 0):   #将一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标\n",
    "    img0, img1, img3 = data\n",
    "    img0, img1, img3 = Variable(img0), Variable(img1), Variable(img3)\n",
    "    output1, output2, output3 = net(img0, img1,img3)   # network output\n",
    "    eu_distance_same = F.pairwise_distance(output1, output2)\n",
    "    eu_dist_same = eu_distance_same.cpu().data.numpy()[0][0]\n",
    "    eu_distance_diff = F.pairwise_distance(output1, output3)\n",
    "    eu_dist_diff = eu_distance_diff.cpu().data.numpy()[0][0]\n",
    "    same_one_valida = np.append(same_one_valida, eu_dist_same)\n",
    "    diff_one_valida = np.append(diff_one_valida, eu_dist_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(same_one_valida, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from same classes')\n",
    "sns.distplot(diff_one_valida, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from different classes')\n",
    "plt.xlabel('Dissimilarity(Euclidean Distance)')\n",
    "plt.ylabel('Relative quantity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_array = np.array([])\n",
    "fpr_array = np.array([])\n",
    "same_one_valida = np.array([])\n",
    "diff_one_valida = np.array([])\n",
    "\n",
    "\n",
    "for thres in range(0,140,2):\n",
    "    print(thres)\n",
    "    tpm = 0\n",
    "    fpm = 0\n",
    "    tp_fn = 0\n",
    "    fp_tn = 0\n",
    "    for i, data in enumerate(valida_distri_dataloader, 0):\n",
    "        img0, img1, img2 = data\n",
    "        img0, img1, img2 = Variable(img0), Variable(img1), Variable(img2)\n",
    "        output1, output2, output3 = net(img0, img1, img2)   # network output\n",
    "        eu_distance_ap = F.pairwise_distance(output1, output2)\n",
    "        eu_dist_ap = eu_distance_ap.cpu().data.numpy()[0][0]\n",
    "        eu_distance_an = F.pairwise_distance(output1, output3)\n",
    "        eu_dist_an = eu_distance_an.cpu().data.numpy()[0][0]\n",
    "        \n",
    "        tp_fn = tp_fn + 1\n",
    "        fp_tn = fp_tn + 1\n",
    "        \n",
    "        if eu_dist_ap < (thres/10):\n",
    "            tpm = tpm + 1\n",
    "            \n",
    "        if eu_dist_an < (thres/10):\n",
    "            fpm = fpm + 1\n",
    "\n",
    "    try:\n",
    "        tpr=tpm/tp_fn\n",
    "    except:\n",
    "        print(\"still 0\")\n",
    "    else:\n",
    "        tpr_array = np.append(tpr_array,tpr)\n",
    "    try:\n",
    "        fpr=fpm/fp_tn\n",
    "    except:\n",
    "        print(\"still 0\")\n",
    "    else:\n",
    "        fpr_array = np.append(fpr_array,fpr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tp_fn,fp_tn,tpm,fpm)\n",
    "len(fpr_array)\n",
    "x = fpr_array\n",
    "y = tpr_array \n",
    "sia_adam_roc = (x,y)\n",
    "sia_adam_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sia_adam_roc[0]\n",
    "y = sia_adam_roc[1]\n",
    "\n",
    "# This is the ROC curve\n",
    "plt.plot(x,y,'+')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# This is the AUC\n",
    "auc = np.trapz(y,x)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "triplet_dataset = TripletNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                       transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                    transforms.ToTensor()\n",
    "                                                                    ]),\n",
    "                                       should_invert=False)\n",
    "\n",
    "test_distri_dataloader = DataLoader(triplet_dataset, \n",
    "                             num_workers=6, \n",
    "                             batch_size=1, \n",
    "                             shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "same_one_test = np.array([])\n",
    "diff_one_test = np.array([])\n",
    "\n",
    "\n",
    "for i, data in enumerate(valida_distri_dataloader, 0):   #将一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标\n",
    "    img0, img1, img3 = data\n",
    "    img0, img1, img3 = Variable(img0), Variable(img1), Variable(img3)\n",
    "    output1, output2, output3 = net(img0, img1,img3)   # network output\n",
    "    eu_distance_same = F.pairwise_distance(output1, output2)\n",
    "    eu_dist_same = eu_distance_same.cpu().data.numpy()[0][0]\n",
    "    eu_distance_diff = F.pairwise_distance(output1, output3)\n",
    "    eu_dist_diff = eu_distance_diff.cpu().data.numpy()[0][0]\n",
    "    same_one_test = np.append(same_one_test, eu_dist_same)\n",
    "    diff_one_test = np.append(diff_one_test, eu_dist_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(same_one_test, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from same classes')\n",
    "sns.distplot(diff_one_test, \n",
    "             #bins=20, \n",
    "             rug=False, \n",
    "             hist=True,\n",
    "            label='Samples from different classes')\n",
    "plt.xlabel('Dissimilarity(Euclidean Distance)')\n",
    "plt.ylabel('Relative quantity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "folder_dataset_test = dset.ImageFolder(root=Config.testing_dir)\n",
    "triplet_dataset = TripletNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                       transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                    transforms.ToTensor()\n",
    "                                                                    ]),\n",
    "                                       should_invert=False)\n",
    "\n",
    "test_dataloader = DataLoader(triplet_dataset, num_workers=6, batch_size=1, shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "x0,_,_ = next(dataiter)\n",
    "\n",
    "threshold = 4.95\n",
    "\n",
    "for i in range(10):\n",
    "    x0, x1, x2 = next(dataiter)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    output1, output2, output3 = net(Variable(x0), Variable(x1),Variable(x2))     #Variable(x0).cuda()\n",
    "    eu_distance = F.pairwise_distance(output1, output2)\n",
    "    eu_dist = eu_distance.cpu().data.numpy()[0][0]\n",
    "    if eu_dist <= threshold:\n",
    "        imshow(torchvision.utils.make_grid(concatenated),\n",
    "           'Dissimilarity:{:.2f}\\nSame One!'.format(eu_dist))\n",
    "    else:\n",
    "        imshow(torchvision.utils.make_grid(concatenated),\n",
    "           'Dissimilarity:{:.2f}\\nDifferent Ones!'.format(eu_dist))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
